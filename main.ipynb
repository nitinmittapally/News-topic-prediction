{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import nltk\n",
    "import os\n",
    "import itertools\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "default_stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('Huff_news.csv', parse_dates = True, index_col = 0)\n",
    "\n",
    "all_data.dropna(inplace = True)\n",
    "\n",
    "all_data[\"text_all\"] = all_data[\"headline\"] +\" \"+ all_data[\"short_description\"]\n",
    "\n",
    "text_all = all_data[\"text_all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc,\n",
    "            rm_punctuation=True,\n",
    "            rm_digits=True,\n",
    "            lemmatize=False,\n",
    "            norm_case=True,\n",
    "            stem=False,\n",
    "            rm_stopwords=True):\n",
    "            \"\"\"\n",
    "            Creates a cleaned list of words with the given options\n",
    "            Python 3 friendly\n",
    "\n",
    "            in:\n",
    "                a single text string (eg representing an article)\n",
    "            out:\n",
    "                same text but cleaned according to given options\n",
    "\n",
    "            \"\"\"\n",
    "            # Doc overall operations\n",
    "            if(rm_digits==True):\n",
    "                table = str.maketrans({key: None for key in string.digits})\n",
    "                doc = str(doc).translate(table)\n",
    "            if(norm_case==True):\n",
    "                doc = doc.lower()\n",
    "            if(rm_punctuation==True):\n",
    "                table = str.maketrans({key: None for key in string.punctuation})\n",
    "                doc = doc.translate(table)\n",
    "            if(rm_stopwords==True):\n",
    "                words = \" \".join([i for i in doc.split() if i not in default_stopwords])\n",
    "            else:\n",
    "                words = \" \".join([i for i in doc.split()])\n",
    "            if(lemmatize==True):\n",
    "                lemma = WordNetLemmatizer()\n",
    "                words = \" \".join(lemma.lemmatize(word) for word in words.split())\n",
    "            if(stem==True):\n",
    "                words = \" \".join(porter_stemmer.stem(word) for word in words.split())\n",
    "            return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [clean_text(x,stem=False,lemmatize=True) for x in all_data.text_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =documents, all_data.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=3800, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4847699287103046\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.49      0.67      0.57        84\n",
      "  BLACK VOICES       0.10      0.45      0.16        11\n",
      "      BUSINESS       0.61      0.52      0.56       250\n",
      "       COLLEGE       0.23      0.53      0.32        30\n",
      "        COMEDY       0.00      0.00      0.00         0\n",
      "         CRIME       0.00      0.00      0.00         0\n",
      "     EDUCATION       0.49      0.66      0.56        53\n",
      " ENTERTAINMENT       0.55      0.57      0.56       137\n",
      "         FIFTY       0.20      0.44      0.28        63\n",
      "     GOOD NEWS       0.00      0.00      0.00         0\n",
      "         GREEN       0.54      0.75      0.63        87\n",
      "HEALTHY LIVING       0.73      0.37      0.49       635\n",
      "        IMPACT       0.28      0.33      0.31       165\n",
      " LATINO VOICES       0.05      1.00      0.10         1\n",
      "         MEDIA       0.06      1.00      0.11         2\n",
      "       PARENTS       0.61      0.48      0.54       236\n",
      "      POLITICS       0.77      0.43      0.56       749\n",
      "  QUEER VOICES       0.48      0.79      0.60        53\n",
      "      RELIGION       0.37      0.74      0.49        57\n",
      "       SCIENCE       0.03      0.33      0.06         3\n",
      "        SPORTS       0.31      0.60      0.40        30\n",
      "         STYLE       0.10      0.75      0.17         4\n",
      "         TASTE       0.75      0.73      0.74       131\n",
      "        TRAVEL       0.55      0.69      0.61       102\n",
      "    WEIRD NEWS       0.00      0.00      0.00         0\n",
      "         WOMEN       0.29      0.41      0.34       108\n",
      "     WORLDPOST       0.24      0.38      0.29        95\n",
      "\n",
      "      accuracy                           0.48      3086\n",
      "     macro avg       0.33      0.50      0.35      3086\n",
      "  weighted avg       0.60      0.48      0.51      3086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logit model\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train, y_train) \n",
    "\n",
    "# test - prediction\n",
    "\n",
    "logistic_prediction = logistic.predict(X_test)\n",
    "\n",
    "\n",
    "#accuracy score\n",
    "print(accuracy_score(logistic_prediction, y_test))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "logit_confusionmatrix=confusion_matrix(logistic_prediction, y_test)\n",
    "\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(logistic_prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TASTE', 'GREEN', 'TRAVEL']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "p,r,f1,s=score(y_test,logistic_prediction )\n",
    "a=pd.Series(f1).sort_values(ascending=False).head(3).index.to_list()\n",
    "all_labels=y.unique()\n",
    "all_labels.sort()\n",
    "list(all_labels[a])\n",
    "# Top 3 categories predicted from logit model with respect to the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24951393389500973\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.22      0.25      0.23       100\n",
      "  BLACK VOICES       0.00      0.00      0.00         5\n",
      "      BUSINESS       0.28      0.23      0.25       253\n",
      "       COLLEGE       0.06      0.15      0.08        27\n",
      "        COMEDY       0.07      0.18      0.10        17\n",
      "         CRIME       0.00      0.00      0.00         1\n",
      "     EDUCATION       0.04      0.12      0.06        25\n",
      " ENTERTAINMENT       0.24      0.26      0.25       133\n",
      "         FIFTY       0.15      0.17      0.16       127\n",
      "     GOOD NEWS       0.00      0.00      0.00         1\n",
      "         GREEN       0.13      0.25      0.17        65\n",
      "HEALTHY LIVING       0.32      0.27      0.29       386\n",
      "        IMPACT       0.22      0.16      0.18       264\n",
      " LATINO VOICES       0.00      0.00      0.00         0\n",
      "         MEDIA       0.03      0.11      0.04         9\n",
      "       PARENTS       0.18      0.21      0.20       162\n",
      "      POLITICS       0.61      0.30      0.40       869\n",
      "  QUEER VOICES       0.28      0.20      0.23       118\n",
      "      RELIGION       0.11      0.17      0.14        76\n",
      "       SCIENCE       0.00      0.00      0.00         3\n",
      "        SPORTS       0.08      0.33      0.14        15\n",
      "         STYLE       0.03      0.20      0.06         5\n",
      "         TASTE       0.32      0.80      0.46        51\n",
      "        TRAVEL       0.21      0.30      0.25        89\n",
      "    WEIRD NEWS       0.00      0.00      0.00         0\n",
      "         WOMEN       0.09      0.12      0.11       113\n",
      "     WORLDPOST       0.28      0.24      0.26       172\n",
      "\n",
      "      accuracy                           0.25      3086\n",
      "     macro avg       0.15      0.19      0.15      3086\n",
      "  weighted avg       0.34      0.25      0.27      3086\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Naive Bayes Classifier\n",
    "\n",
    "# Naive Bayes \n",
    "\n",
    "\n",
    "nb =  GaussianNB()\n",
    "nb.fit(X_train, y_train) \n",
    "\n",
    "nb_prediction = nb.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(accuracy_score(nb_prediction, y_test))\n",
    "\n",
    "\n",
    "nb_confusionmatrix=confusion_matrix(nb_prediction, y_test)\n",
    "\n",
    "#classification report\n",
    "print(classification_report(nb_prediction, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TASTE', 'POLITICS', 'HEALTHY LIVING']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_nb,r_nb,f1_nb,s_nb=score(y_test,nb_prediction )\n",
    "b=pd.Series(f1_nb).sort_values(ascending=False).head(3).index.to_list()\n",
    "all_labels=y.unique()\n",
    "all_labels.sort()\n",
    "list(all_labels[b])\n",
    "# Top 3 predictions from NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4594944912508101\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.44      0.72      0.55        69\n",
      "  BLACK VOICES       0.12      0.75      0.20         8\n",
      "      BUSINESS       0.54      0.55      0.55       209\n",
      "       COLLEGE       0.26      0.55      0.35        33\n",
      "        COMEDY       0.02      1.00      0.04         1\n",
      "         CRIME       0.00      0.00      0.00         0\n",
      "     EDUCATION       0.51      0.66      0.58        56\n",
      " ENTERTAINMENT       0.51      0.56      0.53       129\n",
      "         FIFTY       0.15      0.51      0.24        41\n",
      "     GOOD NEWS       0.00      0.00      0.00         0\n",
      "         GREEN       0.46      0.80      0.58        69\n",
      "HEALTHY LIVING       0.79      0.32      0.45       809\n",
      "        IMPACT       0.26      0.41      0.32       122\n",
      " LATINO VOICES       0.00      0.00      0.00         0\n",
      "         MEDIA       0.03      1.00      0.05         1\n",
      "       PARENTS       0.58      0.52      0.55       209\n",
      "      POLITICS       0.81      0.38      0.52       885\n",
      "  QUEER VOICES       0.44      0.83      0.57        46\n",
      "      RELIGION       0.27      0.72      0.39        43\n",
      "       SCIENCE       0.00      0.00      0.00         0\n",
      "        SPORTS       0.25      0.60      0.36        25\n",
      "         STYLE       0.03      0.33      0.06         3\n",
      "         TASTE       0.65      0.78      0.71       107\n",
      "        TRAVEL       0.45      0.70      0.55        81\n",
      "    WEIRD NEWS       0.00      0.00      0.00         0\n",
      "         WOMEN       0.27      0.46      0.34        89\n",
      "     WORLDPOST       0.14      0.41      0.21        51\n",
      "\n",
      "      accuracy                           0.46      3086\n",
      "     macro avg       0.30      0.50      0.32      3086\n",
      "  weighted avg       0.64      0.46      0.49      3086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC_model = SVC()\n",
    "\n",
    "SVC_model.fit(X_train, y_train)\n",
    "\n",
    "SVC_prediction = SVC_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(SVC_prediction, y_test))\n",
    "\n",
    "\n",
    "svc_confusionmatix=confusion_matrix(SVC_prediction, y_test)\n",
    "\n",
    "\n",
    "print(classification_report(SVC_prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_svc,r_svc,f1_svc,s_svc=score(y_test,SVC_prediction )\n",
    "c=pd.Series(f1_svc).sort_values(ascending=False).head(3).index.to_list()\n",
    "all_labels=y.unique()\n",
    "all_labels.sort()\n",
    "list(all_labels[c])\n",
    "# Top 3 predictions from SVC classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
